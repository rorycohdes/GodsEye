- [ ] populate your readme file with steps for setting up this RAG

- [ ] change docker for production

- [✔️] remove .venv from git history if possible

- [ ] create a branch for different embeddings

- [ ] potentially use google embeddings

- [ ] update the docker file for security

- [✔️] use the compare feature to compare changes between branches of the repository https://github.com/daveebbelaar/pgvectorscale-rag-solution/tree/setup

- [✔️] used interactive mode for data science

- [✔️] messed with the settings so that interactive mode uses the correct kernel

- [✔️] I need to not get "No module named" error in interactive mode. The change to the python path such that is the backend folder fixed the issue

- [ ] Customize the RAG system to take your custom data

- [ ] Write functionality to use relation database

- [✔️] Not scrape companies indefinitely. Maybe for the future. I want to scrape like a hundred and all the data of the company and the return the name of all the companies scrape in that batch for debugging purposes. I want to peridoically scrape a 100 companies. So scrape 100 return data, scrape 100 return data so on

- [✔️] maybe while scraping the terminal could prompt me to return scrape data while it is actively scraping

- [✔️] create a new table or index wtv and then seed the table with data that you would get from ycomb and

- [ ] set up queries to return that data to the front end

- [ ] install requirement.txt file for the newly added packages

- [ ] set up communication between the FE and the BE

- [ ] set up a cron job

- [ ] set up alternative scrapping methods like crawl for ai or vision os sraping

- [ ] more error handling. The beauty with error handling as opposed to logging is you can trace back point of failure of code to a line

- [ ] work on ai interface so that I can activite converse with ai in a pleasing environment

- [ ] ability to add sources

- [ ] supoort for multiple chat sessions

- [✔️] improve the page for the scraped data

- [✔️] notebook lm page shouldn't be defualt url

- [✔️] notebooklm page should go back to the collection of all other notebooks

- [✔️] title rename should reflect in collection

- [✔️] have skeleton cards for cards that don't exist

- [✔️] find design for webscraping cards and notebook cards

- [ ] color coded accents for origin (twitter blue accend in the bottom right corner) of scraped cards

- [ ] get rid of the modules.css and just use tailwind

- [ ] I know I have the company name on the backend so for now I can populate cards with company name

- [✔️] Let the cards have their own page when you click the parent element of the card

- [✔️ ] Use lorem Ipsum to populate the other fields of the card

- [✔️ ] communicate with local deepseek and make the system prompts for each field of the cards

- [ ✔️] a column that stores newly ai created field values

- [ ] find the perfect system prompt for deepseek

- [ ] set up an etl pipeline or use crawl for ai and skip the transform step of etl

- [ ] make your project agentic by using vercel ai sdk

- [ ] make sure that whenever a scrape occurs it's only new companies

- [ ] multi tone accents for scrape sources with multiple color in branding

- [ ] setup scraper so that it only gets latest companies from the scrape

- [ ] setup something like a client for the proxy connections
