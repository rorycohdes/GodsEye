import logging
import time
import json
from typing import Any, List, Optional, Tuple, Union
from datetime import datetime

import pandas as pd
from pydantic import ValidationError

from config.settings import get_settings, CompanySynthesis

from openai import OpenAI
from timescale_vector import client

import cohere

import psycopg


class VectorStore:
    """A class for managing vector operations and database interactions."""

    def __init__(self,table_name: Optional[str] = None):
        """Initialize the VectorStore with settings, OpenAI client, and Timescale Vector client."""
        self.settings = get_settings()
        self.openai_client = OpenAI(api_key=self.settings.openai.api_key)
        self.embedding_model = self.settings.openai.embedding_model
        self.cohere_client = cohere.ClientV2(api_key=self.settings.cohere.api_key)
        self.company_synthesis = self.settings.company_synthesis
        # Initialize DeepSeek client
        self.deepseek_client = OpenAI(
            api_key=self.settings.deepseek.api_key,
            base_url=self.settings.deepseek.base_url
        )
        self.vector_settings = self.settings.vector_store

        # Use provided table_name or fall back to settings
        self.table_name = table_name or self.vector_settings.table_name

        self.vec_client = client.Sync(
            self.settings.database.service_url,
            self.table_name,
            self.vector_settings.embedding_dimensions,
            time_partition_interval=self.vector_settings.time_partition_interval,
        )
    

    def create_keyword_search_index(self):
        """Create a GIN index for keyword search if it doesn't exist."""
        index_name = f"idx_{self.table_name}_contents_gin" # Use dynamic table name
        create_index_sql = f"""
        CREATE INDEX IF NOT EXISTS {index_name}
        ON {self.table_name} USING gin(to_tsvector('english', contents));
        """
        try:
            with psycopg.connect(self.settings.database.service_url) as conn:
                with conn.cursor() as cur:
                    cur.execute(create_index_sql)
                    conn.commit()
                    logging.info(f"GIN index '{index_name}' created or already exists.")
        except Exception as e:
            logging.error(f"Error while creating GIN index: {str(e)}")

    def get_embedding(self, text: str) -> List[float]:
        """
        Generate embedding for the given text.

        Args:
            text: The input text to generate an embedding for.

        Returns:
            A list of floats representing the embedding.
        """
        text = text.replace("\n", " ")
        start_time = time.time()
        embedding = (
            self.openai_client.embeddings.create(
                input=[text],
                model=self.embedding_model,
            )
            .data[0]
            .embedding
        )
        elapsed_time = time.time() - start_time
        logging.info(f"Embedding generated in {elapsed_time:.3f} seconds")
        return embedding

    def create_tables(self) -> None:
        """Create the necessary tablesin the database"""
        self.vec_client.create_tables()

    def create_index(self) -> None:
        """Create the StreamingDiskANN index to spseed up similarity search"""
        self.vec_client.create_embedding_index(client.DiskAnnIndex())

    def drop_index(self) -> None:
        """Drop the StreamingDiskANN index in the database"""
        self.vec_client.drop_embedding_index()

    def upsert(self, df: pd.DataFrame) -> None:
        """
        Insert or update records in the database from a pandas DataFrame.

        Args:
            df: A pandas DataFrame containing the data to insert or update.
                Expected columns: id, metadata, contents, embedding
        """
        records = df.to_records(index=False)
        self.vec_client.upsert(list(records))
        logging.info(
            f"Inserted {len(df)} records into {self.vector_settings.table_name}"
        )

    def semantic_search(
        self,
        query: str,
        limit: int = 5,
        metadata_filter: Union[dict, List[dict]] = None,
        predicates: Optional[client.Predicates] = None,
        time_range: Optional[Tuple[datetime, datetime]] = None,
        return_dataframe: bool = True,
    ) -> Union[List[Tuple[Any, ...]], pd.DataFrame]:
        """
        Query the vector database for similar embeddings based on input text.

        More info:
            https://github.com/timescale/docs/blob/latest/ai/python-interface-for-pgvector-and-timescale-vector.md

        Args:
            query_text: The input text to search for.
            limit: The maximum number of results to return.
            metadata_filter: A dictionary or list of dictionaries for equality-based metadata filtering.
            predicates: A Predicates object for complex metadata filtering.
                - Predicates objects are defined by the name of the metadata key, an operator, and a value.
                - Operators: ==, !=, >, >=, <, <=
                - & is used to combine multiple predicates with AND operator.
                - | is used to combine multiple predicates with OR operator.
            time_range: A tuple of (start_date, end_date) to filter results by time.
            return_dataframe: Whether to return results as a DataFrame (default: True).

        Returns:
            Either a list of tuples or a pandas DataFrame containing the search results.

        Basic Examples:
            Basic search:
                vector_store.semantic_search("What are your shipping options?")
            Search with metadata filter:
                vector_store.semantic_search("Shipping options", metadata_filter={"category": "Shipping"})
        
        Predicates Examples:
            Search with predicates:
                vector_store.semantic_search("Pricing", predicates=client.Predicates("price", ">", 100))
            Search with complex combined predicates:
                complex_pred = (client.Predicates("category", "==", "Electronics") & client.Predicates("price", "<", 1000)) | \
                               (client.Predicates("category", "==", "Books") & client.Predicates("rating", ">=", 4.5))
                vector_store.semantic_search("High-quality products", predicates=complex_pred)
        
        Time-based filtering:
            Search with time range:
                vector_store.semantic_search("Recent updates", time_range=(datetime(2024, 1, 1), datetime(2024, 1, 31)))
        """
        query_embedding = self.get_embedding(query)

        start_time = time.time()

        search_args = {
            "limit": limit,
        }

        if metadata_filter:
            search_args["filter"] = metadata_filter

        if predicates:
            search_args["predicates"] = predicates

        if time_range:
            start_date, end_date = time_range
            search_args["uuid_time_filter"] = client.UUIDTimeRange(start_date, end_date)

        results = self.vec_client.search(query_embedding, **search_args)
        elapsed_time = time.time() - start_time

        self._log_search_time("Vector", elapsed_time)

        if return_dataframe:
            return self._create_dataframe_from_results(results)
        else:
            return results

    def _create_dataframe_from_results(
        self,
        results: List[Tuple[Any, ...]],
    ) -> pd.DataFrame:
        """
        Create a pandas DataFrame from the search results.

        Args:
            results: A list of tuples containing the search results.

        Returns:
            A pandas DataFrame containing the formatted search results.
        """
        # Convert results to DataFrame
        df = pd.DataFrame(
            results, columns=["id", "metadata", "content", "embedding", "distance"]
        )

        # Expand metadata column
        df = pd.concat(
            [df.drop(["metadata"], axis=1), df["metadata"].apply(pd.Series)], axis=1
        )

        # Convert id to string for better readability
        df["id"] = df["id"].astype(str)

        return df

    def delete(
        self,
        ids: List[str] = None,
        metadata_filter: dict = None,
        delete_all: bool = False,
    ) -> None:
        """Delete records from the vector database.

        Args:
            ids (List[str], optional): A list of record IDs to delete.
            metadata_filter (dict, optional): A dictionary of metadata key-value pairs to filter records for deletion.
            delete_all (bool, optional): A boolean flag to delete all records.

        Raises:
            ValueError: If no deletion criteria are provided or if multiple criteria are provided.

        Examples:
            Delete by IDs:
                vector_store.delete(ids=["8ab544ae-766a-11ef-81cb-decf757b836d"])

            Delete by metadata filter:
                vector_store.delete(metadata_filter={"category": "Shipping"})

            Delete all records:
                vector_store.delete(delete_all=True)
        """
        if sum(bool(x) for x in (ids, metadata_filter, delete_all)) != 1:
            raise ValueError(
                "Provide exactly one of: ids, metadata_filter, or delete_all"
            )

        if delete_all:
            self.vec_client.delete_all()
            logging.info(f"Deleted all records from {self.vector_settings.table_name}")
        elif ids:
            self.vec_client.delete_by_ids(ids)
            logging.info(
                f"Deleted {len(ids)} records from {self.vector_settings.table_name}"
            )
        elif metadata_filter:
            self.vec_client.delete_by_metadata(metadata_filter)
            logging.info(
                f"Deleted records matching metadata filter from {self.vector_settings.table_name}"
            )


    def _log_search_time(self, search_type: str, elapsed_time: float) -> None:
        """
        Log the time taken for a search operation.
        Args:
            search_type: The type of search performed (e.g., 'Vector', 'Keyword').
            elapsed_time: The time taken for the search operation in seconds.
        """
        logging.info(f"{search_type} search completed in {elapsed_time:.3f} seconds")

    def keyword_search(
        self, query: str, limit: int = 5, return_dataframe: bool = True
    ) -> Union[List[Tuple[str, str, float]], pd.DataFrame]:
        """
        Perform a keyword search on the contents of the vector store.
        Args:
            query: The search query string.
            limit: The maximum number of results to return. Defaults to 5.
            return_dataframe: Whether to return results as a DataFrame. Defaults to True.
        Returns:
            Either a list of tuples (id, contents, rank) or a pandas DataFrame containing the search results.
        Example:
            results = vector_store.keyword_search("shipping options")
        """
        search_sql = f"""
        SELECT id, contents, ts_rank_cd(to_tsvector('english', contents), query) as rank
        FROM {self.vector_settings.table_name}, websearch_to_tsquery('english', %s) query
        WHERE to_tsvector('english', contents) @@ query
        ORDER BY rank DESC
        LIMIT %s
        """

        start_time = time.time()

        # Create a new connection using psycopg3
        with psycopg.connect(self.settings.database.service_url) as conn:
            with conn.cursor() as cur:
                cur.execute(search_sql, (query, limit))
                results = cur.fetchall()

        elapsed_time = time.time() - start_time
        self._log_search_time("Keyword", elapsed_time)

        if return_dataframe:
            df = pd.DataFrame(results, columns=["id", "content", "rank"])
            df["id"] = df["id"].astype(str)
            return df
        else:
            return results

    def hybrid_search(
        self,
        query: str,
        keyword_k: int = 5,
        semantic_k: int = 5,
        rerank: bool = False,
        top_n: int = 5,
    ) -> pd.DataFrame:
        """
        Perform a hybrid search combining keyword and semantic search results,
        with optional reranking using Cohere.
        Args:
            query: The search query string.
            keyword_k: The number of results to return from keyword search. Defaults to 5.
            semantic_k: The number of results to return from semantic search. Defaults to 5.
            rerank: Whether to apply Cohere reranking. Defaults to True.
            top_n: The number of top results to return after reranking. Defaults to 5.
        Returns:
            A pandas DataFrame containing the combined search results with a 'search_type' column.
        Example:
            results = vector_store.hybrid_search("shipping options", keyword_k=3, semantic_k=3, rerank=True, top_n=5)
        """
        # Perform keyword search
        keyword_results = self.keyword_search(
            query, limit=keyword_k, return_dataframe=True
        )
        keyword_results["search_type"] = "keyword"
        keyword_results = keyword_results[["id", "content", "search_type"]]

        # Perform semantic search
        semantic_results = self.semantic_search(
            query, limit=semantic_k, return_dataframe=True
        )
        semantic_results["search_type"] = "semantic"
        semantic_results = semantic_results[["id", "content", "search_type"]]

        # Combine results
        combined_results = pd.concat(
            [keyword_results, semantic_results], ignore_index=True
        )

        # Remove duplicates, keeping the first occurrence (which maintains the original order)
        combined_results = combined_results.drop_duplicates(subset=["id"], keep="first")

        if rerank:
            return self._rerank_results(query, combined_results, top_n)

        return combined_results

    def _rerank_results(
        self, query: str, combined_results: pd.DataFrame, top_n: int
    ) -> pd.DataFrame:
        """
        Rerank the combined search results using Cohere.
        Args:
            query: The original search query.
            combined_results: DataFrame containing the combined keyword and semantic search results.
            top_n: The number of top results to return after reranking.
        Returns:
            A pandas DataFrame containing the reranked results.
        """
        rerank_results = self.cohere_client.v2.rerank(
            model="rerank-english-v3.0",
            query=query,
            documents=combined_results["content"].tolist(),
            top_n=top_n,
            return_documents=True,
        )

        reranked_df = pd.DataFrame(
            [
                {
                    "id": combined_results.iloc[result.index]["id"],
                    "content": result.document,
                    "search_type": combined_results.iloc[result.index]["search_type"],
                    "relevance_score": result.relevance_score,
                }
                for result in rerank_results.results
            ]
        )

        return reranked_df.sort_values("relevance_score", ascending=False)

    def get_latest_rows(self, limit: int = 1) -> pd.DataFrame:
        """
        Get the most recently added rows from the database.
        
        Args:
            limit: Number of latest rows to return (default: 1 for just the latest)
        
        Returns:
            A pandas DataFrame containing the latest rows
        """
        sql = f"""
            SELECT id, metadata, contents, metadata->>'created_at' as created_at
            FROM {self.table_name}
            ORDER BY id DESC
            LIMIT %s
        """
        
        with psycopg.connect(self.settings.database.service_url) as conn:
            with conn.cursor() as cur:
                cur.execute(sql, (limit,))
                results = cur.fetchall()
        
        df = pd.DataFrame(results, columns=['id', 'metadata', 'contents', 'created_at'])
        df['id'] = df['id'].astype(str)
        
        return df

    def get_latest_with_details(self, limit: int = 1) -> pd.DataFrame:
        """
        Get the latest rows with expanded metadata details.
        
        Args:
            limit: Number of latest rows to return (default: 1)
        
        Returns:
            A pandas DataFrame with expanded metadata columns
        """
        sql = f"""
            SELECT 
                id,
                contents,
                metadata->>'company_name' as company_name,
                metadata->>'location' as location,
                metadata->>'url' as url,
                metadata->>'created_at' as created_at,
                metadata->'ai_insights'->>'pitch' as ai_pitch,
                metadata->'ai_insights'->>'feature_summary' as ai_feature_summary
            FROM {self.table_name}
            ORDER BY id DESC
            LIMIT %s
        """
        
        with psycopg.connect(self.settings.database.service_url) as conn:
            with conn.cursor() as cur:
                cur.execute(sql, (limit,))
                results = cur.fetchall()
        
        return pd.DataFrame(results, columns=[
            'id', 'contents', 'company_name', 'location', 'url', 'created_at', 
            'ai_pitch', 'ai_feature_summary'
        ])

    def get_companies_with_ai_insights(self, limit: int = 10) -> pd.DataFrame:
        """
        Get companies with their AI-generated insights (pitch and feature summary).
        
        Args:
            limit: Number of companies to return (default: 10)
        
        Returns:
            A pandas DataFrame with company details and AI insights
        """
        sql = f"""
            SELECT 
                id,
                metadata->>'company_name' as company_name,
                metadata->>'location' as location,
                metadata->>'url' as url,
                metadata->'ai_insights'->>'pitch' as pitch,
                metadata->'ai_insights'->>'feature_summary' as feature_summary,
                metadata->>'created_at' as created_at
            FROM {self.table_name}
            WHERE metadata->'ai_insights' IS NOT NULL
            ORDER BY id DESC
            LIMIT %s
        """
        
        with psycopg.connect(self.settings.database.service_url) as conn:
            with conn.cursor() as cur:
                cur.execute(sql, (limit,))
                results = cur.fetchall()
        
        return pd.DataFrame(results, columns=[
            'id', 'company_name', 'location', 'url', 'pitch', 'feature_summary', 'created_at'
        ])

    def generate_ai_insights(self, content: str) -> dict:
        """
        Generate pitch and feature summary using DeepSeek on Groq Cloud.
        
        Args:
            content (str): The company content to analyze
            
        Returns:
            dict: Dictionary containing 'pitch' and 'feature_summary' fields
        """
        try:
            system_prompt = f"""
            You are a product catalog assistant. Analyze the following company information and generate insights.
            
            Company Information: {content}
            
            Respond with a valid JSON object that matches this exact structure:
            {{
                "pitch": "A compelling 2-sentence company pitch",
                "feature_summary": ["feature1", "feature2", "feature3"]
            }}
            
            Your response should ONLY contain the JSON object and nothing else.
            """
            
            logging.info(f"Generating AI insights using DeepSeek on Groq Cloud for content: {content[:100]}...")
            
            response = self.deepseek_client.chat.completions.create(
                model=self.settings.deepseek.default_model,
                messages=[{"role": "user", "content": system_prompt}],
                response_format={"type": "json_object"},
                temperature=self.settings.deepseek.temperature,
                max_tokens=self.settings.deepseek.max_tokens
            )
            
            logging.info(f"DeepSeek API response received successfully")
            
            # Parse the JSON response
            response_text = response.choices[0].message.content.strip()
            logging.info(f"Raw response from DeepSeek: {response_text}")
            
            # Parse JSON from API response
            result = json.loads(response_text)
            logging.info(f"Parsed JSON result: {result}")

            # Validate using Pydantic schema
            #If the validation fails (missing fields, wrong data types, etc.), Pydantic raises a ValidationError, which is caught by the except ValidationError block in the code.
            company_synthesis = CompanySynthesis(**result)
            logging.info(f"Pydantic validation successful")

            # Return as dictionary for consistency with existing code
            final_result = {
                "pitch": company_synthesis.pitch,
                "feature_summary": company_synthesis.feature_summary
            }
            
            logging.info(f"Successfully generated AI insights: {final_result}")
            return final_result
            
        except json.JSONDecodeError as e:
            logging.error(f"JSON parsing error with DeepSeek response: {str(e)}")
            logging.error(f"Failed to parse response: {response_text if 'response_text' in locals() else 'No response received'}")
            return {
                "pitch": "JSON parsing failed - invalid response format",
                "feature_summary": ["JSON parsing failed"]
            }
            
        except ValidationError as e:
            logging.error(f"Pydantic validation error with DeepSeek response: {str(e)}")
            logging.error(f"Response did not match CompanySynthesis schema: {result if 'result' in locals() else 'No parsed result'}")
            return {
                "pitch": "Schema validation failed - response structure invalid",
                "feature_summary": ["Schema validation failed"]
            }
            
        except Exception as e:
            logging.error(f"Error generating AI insights with DeepSeek on Groq Cloud: {str(e)}")
            logging.error(f"Error type: {type(e).__name__}")
            if hasattr(e, 'response') and hasattr(e.response, 'text'):
                logging.error(f"API Error response: {e.response.text}")
            return {
                "pitch": f"AI generation failed: {str(e)}",
                "feature_summary": [f"AI generation failed: {str(e)}"]
            }